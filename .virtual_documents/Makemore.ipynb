# Reading the names from the text file into a python list
words = open('names.txt', 'r').read().splitlines()


words[:10]


len(words)


# get the shorted name length
min(len(w) for w in words)


# get the longest name length
max(len(w) for w in words)





# we use bigram modelling using probabilities of two letters ocurring next to each other by counting the occurrences in our dataset. 


# Dictionary to hold the bigrams and the count
b = {}
for w in words:
    chs = ['<S>'] + list(w) + ['<E>']
    for ch1, ch2 in zip(chs, chs[1:]):
        bigram = (ch1, ch2)
        b[bigram] = b.get(bigram, 0) + 1
        


# Sort the counted bigrams from highest to lowest
sorted(b.items(), key = lambda kv: -kv[1])


import torch


N = torch.zeros((27, 27), dtype = torch.int32) 


chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi['.'] = 0



for w in words:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        firstInt = stoi[ch1]
        secondInt = stoi[ch2]
        N[firstInt, secondInt] += 1
        


import matplotlib.pyplot as plt
%matplotlib inline


plt.imshow(N)


itos = {i:s for s,i in stoi.items()}


plt.figure(figsize=(16, 16))
plt.imshow(N, cmap='Blues')
for i in range(27):
    for j in range(27):
        chstr = itos[i] + itos[j]
        plt.text(j, i, chstr, ha="center", va="bottom", color='gray')
        plt.text(j, i, N[i, j].item(), ha="center", va="top", color='gray')
plt.axis('off')


N[0]


P = (N+1).float()
P /= P.sum(1, keepdim=True)


g = torch.Generator().manual_seed(2147483647)

out = []
ix = 0
while True:
    # p = N[ix].float()
    # p = p / p.sum()
    p = P[ix]
    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
    out.append(itos[ix])
    if ix == 0:
        break

print(''.join(out))


# GOAL: maximize likelihood of the data w.r.t model parameters (statistical modelling)
# equivalent to maximizing the log likelihood (because log is monotonic)
# equivalent to minimizing the negative log likelihood
# equivalent to minimizing the average log likelihood

# log(a*b*c) = log(a) + log(b) + log(c)


P = (N+1).float()
P /= P.sum(1, keepdim=True)


log_likelihood = 0.0
n = 0

for w in ["andrejq"]:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        prob = P[ix1, ix2]
        logprob = torch.log(prob)
        log_likelihood += logprob
        n += 1
        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')

print(f'{log_likelihood=}')
nll = -log_likelihood
print(f'{nll=}')
print(f'{nll/n}')








# Creating training set of bigrams (x,y)
xs, ys = [], []

for w in words[:1]:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        print(ch1, ch2)
        xs.append(ix1)
        ys.append(ix2)

xs = torch.tensor(xs)
ys = torch.tensor(ys)


xs



